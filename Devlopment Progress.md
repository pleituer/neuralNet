# Current Version: v0.3.0

## v0.3.0 (Feb 17, 2023)
### Added

- Convolutional Neural Networks
- Pooling Layers
- Save & Load functions
- New example [Solving_MNIST](https://github.com/pleituer/neuralNet/tree/main/examples/Solving_MNIST)

## v0.2.3 (Feb 14, 2023)
### Added

- How-To's in each of the examples

### Fixed

- fixed the wrong implementation of SELU and PReLU

## v0.2.2 (Feb 14, 2023)
### Added

- the following activation layers:
  - Identity
  - Binary Step
  - GELU
  - Softplus
  - ELU
  - SELU
  - Leaky ReLU
  - PReLU
  - SiLU
  - Gaussian
- a new example ([Tic Tac Toe AI](https://github.com/pleituer/neuralNet/tree/main/examples/Tic%20Tac%20Toe))

### Fixed

- a bug where setting `test=False` will return an error

## v0.2.1 (Feb 10, 2023)
### Added

- testing for RNNs
- Gated Recurrent Units

## v0.2.0 (Feb 10, 2023)
### Added

- testing for FFNNs
- visualizing for FFNNs
- an new example ([XOR](https://github.com/pleituer/neuralNet/tree/main/examples/XOR))

### Fixed

the visualizer (previously `test` function)

## v0.1 (Feb 8, 2023)
### Added

- Dense Layer
- Activations (tanh, sigmoid, ReLu, softmax)
- GRUs (a simplistic one with one internal weight and one internal activation function, tanh, and its called `SGRU`)
- LSTM (still in progress of debugging)
- FFNNs
- RNNs
